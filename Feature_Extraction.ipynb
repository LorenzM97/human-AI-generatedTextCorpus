{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e09d025",
   "metadata": {},
   "source": [
    "# Feature Derivation for the Different Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac7e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.helper_functions as helper_functions\n",
    "from collections import Counter\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a334e5c8",
   "metadata": {},
   "source": [
    "### Load Dataset Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140880aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki or news\n",
    "domain = \"wiki\"\n",
    "\n",
    "human_features_df =  pd.read_pickle(\"{}_human_generated.pkl\".format(domain))\n",
    "gpt_features_df =  pd.read_pickle(\"{}_chatgpt_generated.pkl\".format(domain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b4165",
   "metadata": {},
   "source": [
    "# Derive TF-IDF Feature\n",
    "Process: TF-IDF derived for human-generated texts and subset of AI-generated texts (e.g., basic AI-rephrased texts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729cf9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_top_indices_and_vectorizer(texts):\n",
    "    # Initialize the TfidfVectorizer with uni- and bigram options\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    \n",
    "    # Fit the vectorizer on the texts\n",
    "    tfidf.fit(texts)\n",
    "    \n",
    "    # Get the feature names (uni- and bigrams)\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "    # Get the document-term matrix (DTM) of the corpus\n",
    "    dtm = tfidf.transform(texts)\n",
    "\n",
    "    # Get the sum of the tf-idf scores for each feature across all documents\n",
    "    sum_tfidf = dtm.sum(axis=0)\n",
    "\n",
    "    # Convert the DTM to a dense matrix for easier manipulation\n",
    "    dense_dtm = dtm.todense()\n",
    "\n",
    "    # Get the indices of the top 500 features with the highest tf-idf scores\n",
    "    top_indices = sum_tfidf.argsort()[0, -500:]\n",
    "    top_indices = top_indices.tolist()[0]\n",
    "    top_features = []\n",
    "    # Get the feature names (uni- and bigrams) of the top 500 features\n",
    "    for top_val in top_indices:\n",
    "        top_features.append(feature_names[top_val])\n",
    "        \n",
    "    return tfidf, top_indices, top_features\n",
    "\n",
    "def calc_tfidf(text, vectorizer, top_indices):\n",
    "    # Transform the new document into a DTM\n",
    "    new_dtm = vectorizer.transform([text])\n",
    "\n",
    "    # Get the tf-idf scores for the top 500 features of the new document\n",
    "    new_tfidf = [new_dtm[0, i] for i in top_indices]\n",
    "    \n",
    "    return new_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b06ab",
   "metadata": {},
   "source": [
    "#### Define Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = human_features_df.copy()\n",
    "gpt_df = gpt_features_df.copy()\n",
    "gpt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb528d",
   "metadata": {},
   "source": [
    "#### Define Type of AI-written text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_source = \"generated_base\"\n",
    "language = \"fr\"\n",
    "\n",
    "main_df = main_df[main_df[\"language\"] == language]\n",
    "gpt_df = gpt_df[gpt_df[\"language\"] == language]\n",
    "\n",
    "gpt_filtered = gpt_df[gpt_df[\"source\"] == gpt_source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f5b93-7cdb-4422-b820-b47736d7b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c886ee",
   "metadata": {},
   "source": [
    "#### Combine human-generated and AI-written texts\n",
    "Should be 100 human- and 100 AI-texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073106e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = main_df.text.tolist() + gpt_filtered.text.tolist()\n",
    "len(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc1642",
   "metadata": {},
   "source": [
    "#### Get 500 top uni- and bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49beb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf, top_indices, top_features = get_top_indices_and_vectorizer(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eb2da3",
   "metadata": {},
   "source": [
    "#### Calculate TF-IDF per text for human-generated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a582cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df[\"tfidf_{}\".format(gpt_source)] = main_df.text.apply(lambda x: calc_tfidf(x, tfidf, top_indices))\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60184b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_df.to_pickle(\"Data/de_wiki_features_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5f6f4",
   "metadata": {},
   "source": [
    "#### Calculate TF-IDF per text for AI-generated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column \"tiidf\" if it does not exist yet\n",
    "gpt_filtered[\"tfidf\"] = None\n",
    "\n",
    "for index, row in gpt_filtered.iterrows():\n",
    "    if row.source == gpt_source:\n",
    "        gpt_filtered.at[index, \"tfidf\"] = calc_tfidf(row.text, tfidf, top_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb4b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt_df.to_pickle(\"Data/de_gpt_features_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d489a0",
   "metadata": {},
   "source": [
    "## Create Sub-Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f208f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df for language only\n",
    "#language = \"en\"\n",
    "human_final = main_df[main_df['language'] == language]\n",
    "\n",
    "gpt_final = gpt_filtered[gpt_filtered['language'] == language]\n",
    "\"\"\"\n",
    "gpt_en_rephr_b = news_feature_df[news_feature_df[\"source\"] == \"rephrase_base\"]\n",
    "gpt_en_rephr_e = news_feature_df[news_feature_df[\"source\"] == \"rephrase_expert\"]\n",
    "gpt_en_gen_b = news_feature_df[news_feature_df[\"source\"] == \"generated_base\"]\n",
    "gpt_en_gen_e = news_feature_df[news_feature_df[\"source\"] == \"generated_expert\"]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29d2e4-7ece-4716-a118-b5846390dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab60f9",
   "metadata": {},
   "source": [
    "## Create Feature DFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c22d7b",
   "metadata": {},
   "source": [
    "### Define DF to create feature for\n",
    "\n",
    "Note: Create DF either for human OR AI generated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009aa68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = human_final.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac290b81",
   "metadata": {},
   "source": [
    "### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ef6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"fr\"\n",
    "lang_tool_lang = \"fr-FR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4669505",
   "metadata": {},
   "source": [
    "### Derive Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfbf44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# FEATURE ChatGPT ANSWER\n",
    "df = helper_functions.ordinal_gpt_feature(df)\n",
    "\n",
    "\n",
    "df['character_count'] = df.text.str.len()\n",
    "df['words_count'] = df.text.apply(lambda x: len(str(x).split(' ')))\n",
    "\n",
    "# FEATURE TITLE OCCURENCE\n",
    "df = helper_functions.title_occurence(df)\n",
    "\n",
    "# FEATURES FOR OCCURENCE OF WORDS\n",
    "#df = helper_functions.count_word_occurence(df, [\"the\", \"it\", \"is\", \"nevertheless\", \"although\", \"however\", \"therefore\"], add_blanks=True)\n",
    "\n",
    "# FEATURE FOR NUMBER OF SENTENCES\n",
    "df['sentence_count'] = helper_functions.count_sentences_raw_text(df, \"hybrid\")\n",
    "\n",
    "# FEATURE AVERAGE NUMBER OF WORDS PER SENTENCE\n",
    "#df[\"avg_words_per_sentence\"] = helper_functions.words_per_sentence(df)\n",
    "\n",
    "# FEATURE COUNT OF QUOTATION MARKS\n",
    "df['quotation_count'] = df['text'].str.count('\\\"')\n",
    "\n",
    "# FEATURE COUNT OF UNIQUE WORDS ABSOLUTE\n",
    "df[\"unique_words_count\"] = df.text.apply(lambda x: len(Counter(re.sub(r'[^A-Za-z \\n]', '', x).lower().split())))\n",
    "\n",
    "# FEATURE COUNT OF UNIQUE WORDS ABSOLUTE RELATIVE TO ALL WORDS IN TEXT\n",
    "df[\"unique_words_relative\"] = df[\"unique_words_count\"] / df[\"words_count\"]\n",
    "\n",
    "# FEATURE COUNT OF SPECIAL CHARACTERS\n",
    "pattern = r'[0-9a-z.?¿!¡,\\n çñáãâàîïíìóôòéèêúûùäöüß]'  # those are excluded from count -> removed from text\n",
    "df[\"special_char_count\"] = df.text.apply(lambda x: len(re.sub(pattern,'', x.lower())))\n",
    "\n",
    "df = helper_functions.add_flesch_scores(df)\n",
    "\n",
    "df[\"personal_pronoun_relative\"] = df.text.apply(lambda x: helper_functions.count_personal_pronouns(x, \"rel\", lang))\n",
    "df[\"personal_pronoun_count\"] = df.text.apply(lambda x: helper_functions.count_personal_pronouns(x, \"abs\", lang))\n",
    "\n",
    "df[\"stats\"] = df[\"text\"].apply(helper_functions.calculate_paragraph_stats)\n",
    "df[[\"words_per_paragraph_mean\", \"words_per_paragraph_stdev\", \"sentences_per_paragraph_mean\", \"sentences_per_paragraph_stdev\"]] = pd.DataFrame(df[\"stats\"].tolist(), index=df.index)\n",
    "# Drop the original 'stats' column\n",
    "df.drop(columns=[\"stats\"], inplace=True)\n",
    "df[\"punctuation_count\"] = df.text.apply(helper_functions.count_punctuation)\n",
    "\n",
    "df[\"paragraph_count\"] = df.text.apply(helper_functions.count_paragraphs)\n",
    "\n",
    "df[\"pos_per_sentence_mean\"] = df.text.apply(lambda x: helper_functions.get_avg_pos_types(x, lang))\n",
    "\n",
    "df[\"stats\"] = df[\"text\"].apply(helper_functions.get_sentence_stats)\n",
    "df[[\"unique_words_per_sentence_mean\", \"unique_words_per_sentence_stdev\", \"words_per_sentence_mean\", \"words_per_sentence_stdev\"]] = pd.DataFrame(df[\"stats\"].tolist(), index=df.index)\n",
    "# Drop the original 'stats' column\n",
    "df.drop(columns=[\"stats\"], inplace=True)\n",
    "\n",
    "df[\"uppercase_letters_relative\"] = df.text.apply(helper_functions.uppercase_percentage)\n",
    "df[\"discourse_marker_count\"] = df.text.apply(lambda x: helper_functions.discourse_marker_count(x, lang))\n",
    "df[\"stop_word_count\"] = df.text.apply(lambda x: helper_functions.count_stopwords(x, lang))\n",
    "df[\"multi_blank_count\"] = df.text.apply(helper_functions.count_double_blanks)\n",
    "\n",
    "\n",
    "import language_tool_python\n",
    "\n",
    "if lang == \"en\" or lang == \"fr\" or lang == \"es\":\n",
    "    # FEATURE LANGUAGE FINDINGS\n",
    "    tool = language_tool_python.LanguageTool(lang_tool_lang)\n",
    "    df['grammar_error_count'] = df.text.apply(lambda x: len(tool.check(x)))\n",
    "else:\n",
    "    tool = language_tool_python.LanguageToolPublicAPI(lang_tool_lang)\n",
    "    df[\"grammar_error_count\"] = None\n",
    "    for index, row in df.iterrows():\n",
    "        print(index)\n",
    "        sentences = sent_tokenize(row.text)\n",
    "        error_count = 0\n",
    "        for sentence in sentences:\n",
    "            error_count += len(tool.check(sentence))\n",
    "        df.at[index, 'grammar_error_count'] = error_count\n",
    "\n",
    "df[\"sentiment\"] = df.text.apply(lambda x: helper_functions.get_sentiment(x, lang))\n",
    "df[[\"sentiment_polarity\", \"sentiment_subjectivity\"]] = pd.DataFrame(df[\"sentiment\"].tolist(), index=df.index)\n",
    "# Drop the original 'sentiment' column\n",
    "df.drop(columns=[\"sentiment\"], inplace=True)\n",
    "\n",
    "\n",
    "# FEATURE PERPLEXITY\n",
    "df = helper_functions.add_perplexity(df, lang)\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "df[\"sent_vec_stats\"] = df.text.apply(lambda x: helper_functions.sentence_vector_mean_vector_and_distance(x, model))\n",
    "df[[\"sentence_bert\", \"sentence_bert_dist\"]] = pd.DataFrame(df[\"sent_vec_stats\"].tolist(), index=df.index)\n",
    "df.drop(columns=[\"sent_vec_stats\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93717c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4887139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle(\"Data/en_gpt_features_df.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
